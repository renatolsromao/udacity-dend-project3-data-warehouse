# Sparkify Analytics Database & ETL

## Objective

Provide a functional database on cloud with songs users are listening to,
so the analytics team can work with.

## Datasets

The data used in the ETL pipeline is from the original files 
(in JSON), generated by the music streaming app. There are two
datasets, described bellow:

- Songs Dataset: Contains metadata about a song and the artist
of that song.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

- Log Dataset: Activity from the music app. 

## Running 

To run the project you are going to need Pipenv. Pipenv is used 
so you don't need to install the dependencies locally. 

```commandline
$ pipenv install
$ pipenv run python <file.py>
```

## Starting

There are two main scripts to run in order to create the database
and importing the data. 

The first one, "create_tables.py", will
drop the tables if they exists and re-creating them. 

```commandline
$ pipenv run python create_tables.py
```

The second one, "etl.py", will load song and log data from the 
Sparkify s3 bucket and insert this date properly, first in the
staging tables an then in the final Star schema tables.

```commandline
$ pipenv run python etl.py
```

Both scripts use "sql_queries.py" in order to achieve the result.


## Setting up the Redshift cluster

Use "iac.py" script to creates a Redshift Cluster,
according to the dwh.cfg file. Remember to teardown the Cluster
after usage.
